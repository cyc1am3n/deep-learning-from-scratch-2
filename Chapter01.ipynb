{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 신경망의 추론\n",
    "#### 1.2.1 신경망 추론 전체 그림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10, 2) # 입력\n",
    "W1 = np.random.randn(2, 4) # 가중치\n",
    "b1 = np.random.randn(4)    # 편향\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.matmul(x, W1) +  b1\n",
    "a = sigmoid(h)\n",
    "s = np.matmul(a, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 계층으로 클래스화 및 순전파 구현\n",
    "모듈화를 통해 신경망을 구축해보자. 다음 구현 규칙을 가지는 클래스를 만들자.\n",
    "* 모든 계층은 `forward()`와 `backward()` 메서드를 가진다.\n",
    "* 모든 계층은 인스턴스 변수인 `params`와 `grads`를 가진다.  \n",
    "-> `forward()`와 `backward()`는 각각 순전파, 역전파를 의미하고,  \n",
    "`grads`는 `params`에 저장된 각 매개변수에 대응하여 해당 매개변수의 기울기를 보관하는 리스트이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.09791651 -0.73924743  0.86570786]\n",
      " [ 3.73719873 -0.1282783   1.11797607]\n",
      " [ 3.37007981 -0.45637231  0.47660684]\n",
      " [ 3.61720696 -0.21975963  0.97398052]\n",
      " [ 3.59430932 -0.33326984  0.2195894 ]\n",
      " [ 3.68638811 -0.24082458  1.26645088]\n",
      " [ 3.39762937 -0.43573645  1.01761019]\n",
      " [ 3.62928133 -0.25401453  1.19495751]\n",
      " [ 3.76382074 -0.28568211 -0.13801393]\n",
      " [ 3.69378386 -0.31062828 -0.02107396]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 신경망의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_lile(W)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.matmul(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        self.grads[0][...] = dW       # grads에 dW를 깊은 복사\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grad[0][...] = dW\n",
    "        self.grad[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size\n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 벡터)    \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치 갱신\n",
    "* 1단계: 미니배치  \n",
    "훈련 데이터 중에서 무작위로 다수의 데이터를 골라낸다.  \n",
    "* 2단계: 기울기 계산  \n",
    "오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기를 구한다.\n",
    "* 3단계: 매개변수 갱신  \n",
    "기울기를 사용하여 가중치 매개변수를 갱신한다.\n",
    "* 4단계: 반복  \n",
    "1~3단계를 필요한 만큼 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr + grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise (psuedo-code)\n",
    "model = TwoLayerNet(...)\n",
    "\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    x_batch, t_batch = get_mini_batch(...) # 미니배치 획득\n",
    "    loss = model.forward(x_batch, t_batch)\n",
    "    model.backward()\n",
    "    optimizer.update(model.params, model.grads)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 신경망으로 문제를 풀다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 2)\n",
      "t (300, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "print('x', x.shape) # (300, 2)\n",
    "print('t', t.shape) # (300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치의 편향 초기화\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 10 / 10 | 손실 1.13\n",
      "| 에폭 2 | 반복 10 / 10 | 손실 1.13\n",
      "| 에폭 3 | 반복 10 / 10 | 손실 1.12\n",
      "| 에폭 4 | 반복 10 / 10 | 손실 1.12\n",
      "| 에폭 5 | 반복 10 / 10 | 손실 1.11\n",
      "| 에폭 6 | 반복 10 / 10 | 손실 1.14\n",
      "| 에폭 7 | 반복 10 / 10 | 손실 1.16\n",
      "| 에폭 8 | 반복 10 / 10 | 손실 1.11\n",
      "| 에폭 9 | 반복 10 / 10 | 손실 1.12\n",
      "| 에폭 10 | 반복 10 / 10 | 손실 1.13\n",
      "| 에폭 11 | 반복 10 / 10 | 손실 1.12\n",
      "| 에폭 12 | 반복 10 / 10 | 손실 1.11\n",
      "| 에폭 13 | 반복 10 / 10 | 손실 1.09\n",
      "| 에폭 14 | 반복 10 / 10 | 손실 1.08\n",
      "| 에폭 15 | 반복 10 / 10 | 손실 1.04\n",
      "| 에폭 16 | 반복 10 / 10 | 손실 1.03\n",
      "| 에폭 17 | 반복 10 / 10 | 손실 0.96\n",
      "| 에폭 18 | 반복 10 / 10 | 손실 0.92\n",
      "| 에폭 19 | 반복 10 / 10 | 손실 0.92\n",
      "| 에폭 20 | 반복 10 / 10 | 손실 0.87\n",
      "| 에폭 21 | 반복 10 / 10 | 손실 0.85\n",
      "| 에폭 22 | 반복 10 / 10 | 손실 0.82\n",
      "| 에폭 23 | 반복 10 / 10 | 손실 0.79\n",
      "| 에폭 24 | 반복 10 / 10 | 손실 0.78\n",
      "| 에폭 25 | 반복 10 / 10 | 손실 0.82\n",
      "| 에폭 26 | 반복 10 / 10 | 손실 0.78\n",
      "| 에폭 27 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 28 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 29 | 반복 10 / 10 | 손실 0.78\n",
      "| 에폭 30 | 반복 10 / 10 | 손실 0.75\n",
      "| 에폭 31 | 반복 10 / 10 | 손실 0.78\n",
      "| 에폭 32 | 반복 10 / 10 | 손실 0.77\n",
      "| 에폭 33 | 반복 10 / 10 | 손실 0.77\n",
      "| 에폭 34 | 반복 10 / 10 | 손실 0.78\n",
      "| 에폭 35 | 반복 10 / 10 | 손실 0.75\n",
      "| 에폭 36 | 반복 10 / 10 | 손실 0.74\n",
      "| 에폭 37 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 38 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 39 | 반복 10 / 10 | 손실 0.73\n",
      "| 에폭 40 | 반복 10 / 10 | 손실 0.75\n",
      "| 에폭 41 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 42 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 43 | 반복 10 / 10 | 손실 0.76\n",
      "| 에폭 44 | 반복 10 / 10 | 손실 0.74\n",
      "| 에폭 45 | 반복 10 / 10 | 손실 0.75\n",
      "| 에폭 46 | 반복 10 / 10 | 손실 0.73\n",
      "| 에폭 47 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 48 | 반복 10 / 10 | 손실 0.73\n",
      "| 에폭 49 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 50 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 51 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 52 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 53 | 반복 10 / 10 | 손실 0.74\n",
      "| 에폭 54 | 반복 10 / 10 | 손실 0.74\n",
      "| 에폭 55 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 56 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 57 | 반복 10 / 10 | 손실 0.71\n",
      "| 에폭 58 | 반복 10 / 10 | 손실 0.70\n",
      "| 에폭 59 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 60 | 반복 10 / 10 | 손실 0.70\n",
      "| 에폭 61 | 반복 10 / 10 | 손실 0.71\n",
      "| 에폭 62 | 반복 10 / 10 | 손실 0.72\n",
      "| 에폭 63 | 반복 10 / 10 | 손실 0.70\n",
      "| 에폭 64 | 반복 10 / 10 | 손실 0.71\n",
      "| 에폭 65 | 반복 10 / 10 | 손실 0.73\n",
      "| 에폭 66 | 반복 10 / 10 | 손실 0.70\n",
      "| 에폭 67 | 반복 10 / 10 | 손실 0.71\n",
      "| 에폭 68 | 반복 10 / 10 | 손실 0.69\n",
      "| 에폭 69 | 반복 10 / 10 | 손실 0.70\n",
      "| 에폭 70 | 반복 10 / 10 | 손실 0.71\n",
      "| 에폭 71 | 반복 10 / 10 | 손실 0.68\n",
      "| 에폭 72 | 반복 10 / 10 | 손실 0.69\n",
      "| 에폭 73 | 반복 10 / 10 | 손실 0.67\n",
      "| 에폭 74 | 반복 10 / 10 | 손실 0.68\n",
      "| 에폭 75 | 반복 10 / 10 | 손실 0.67\n",
      "| 에폭 76 | 반복 10 / 10 | 손실 0.66\n",
      "| 에폭 77 | 반복 10 / 10 | 손실 0.69\n",
      "| 에폭 78 | 반복 10 / 10 | 손실 0.64\n",
      "| 에폭 79 | 반복 10 / 10 | 손실 0.68\n",
      "| 에폭 80 | 반복 10 / 10 | 손실 0.64\n",
      "| 에폭 81 | 반복 10 / 10 | 손실 0.64\n",
      "| 에폭 82 | 반복 10 / 10 | 손실 0.66\n",
      "| 에폭 83 | 반복 10 / 10 | 손실 0.62\n",
      "| 에폭 84 | 반복 10 / 10 | 손실 0.62\n",
      "| 에폭 85 | 반복 10 / 10 | 손실 0.61\n",
      "| 에폭 86 | 반복 10 / 10 | 손실 0.60\n",
      "| 에폭 87 | 반복 10 / 10 | 손실 0.60\n",
      "| 에폭 88 | 반복 10 / 10 | 손실 0.61\n",
      "| 에폭 89 | 반복 10 / 10 | 손실 0.59\n",
      "| 에폭 90 | 반복 10 / 10 | 손실 0.58\n",
      "| 에폭 91 | 반복 10 / 10 | 손실 0.56\n",
      "| 에폭 92 | 반복 10 / 10 | 손실 0.56\n",
      "| 에폭 93 | 반복 10 / 10 | 손실 0.54\n",
      "| 에폭 94 | 반복 10 / 10 | 손실 0.53\n",
      "| 에폭 95 | 반복 10 / 10 | 손실 0.53\n",
      "| 에폭 96 | 반복 10 / 10 | 손실 0.52\n",
      "| 에폭 97 | 반복 10 / 10 | 손실 0.51\n",
      "| 에폭 98 | 반복 10 / 10 | 손실 0.50\n",
      "| 에폭 99 | 반복 10 / 10 | 손실 0.48\n",
      "| 에폭 100 | 반복 10 / 10 | 손실 0.48\n",
      "| 에폭 101 | 반복 10 / 10 | 손실 0.46\n",
      "| 에폭 102 | 반복 10 / 10 | 손실 0.45\n",
      "| 에폭 103 | 반복 10 / 10 | 손실 0.45\n",
      "| 에폭 104 | 반복 10 / 10 | 손실 0.44\n",
      "| 에폭 105 | 반복 10 / 10 | 손실 0.44\n",
      "| 에폭 106 | 반복 10 / 10 | 손실 0.41\n",
      "| 에폭 107 | 반복 10 / 10 | 손실 0.40\n",
      "| 에폭 108 | 반복 10 / 10 | 손실 0.41\n",
      "| 에폭 109 | 반복 10 / 10 | 손실 0.40\n",
      "| 에폭 110 | 반복 10 / 10 | 손실 0.40\n",
      "| 에폭 111 | 반복 10 / 10 | 손실 0.38\n",
      "| 에폭 112 | 반복 10 / 10 | 손실 0.38\n",
      "| 에폭 113 | 반복 10 / 10 | 손실 0.36\n",
      "| 에폭 114 | 반복 10 / 10 | 손실 0.37\n",
      "| 에폭 115 | 반복 10 / 10 | 손실 0.35\n",
      "| 에폭 116 | 반복 10 / 10 | 손실 0.34\n",
      "| 에폭 117 | 반복 10 / 10 | 손실 0.34\n",
      "| 에폭 118 | 반복 10 / 10 | 손실 0.34\n",
      "| 에폭 119 | 반복 10 / 10 | 손실 0.33\n",
      "| 에폭 120 | 반복 10 / 10 | 손실 0.34\n",
      "| 에폭 121 | 반복 10 / 10 | 손실 0.32\n",
      "| 에폭 122 | 반복 10 / 10 | 손실 0.32\n",
      "| 에폭 123 | 반복 10 / 10 | 손실 0.31\n",
      "| 에폭 124 | 반복 10 / 10 | 손실 0.31\n",
      "| 에폭 125 | 반복 10 / 10 | 손실 0.30\n",
      "| 에폭 126 | 반복 10 / 10 | 손실 0.30\n",
      "| 에폭 127 | 반복 10 / 10 | 손실 0.28\n",
      "| 에폭 128 | 반복 10 / 10 | 손실 0.28\n",
      "| 에폭 129 | 반복 10 / 10 | 손실 0.28\n",
      "| 에폭 130 | 반복 10 / 10 | 손실 0.28\n",
      "| 에폭 131 | 반복 10 / 10 | 손실 0.27\n",
      "| 에폭 132 | 반복 10 / 10 | 손실 0.27\n",
      "| 에폭 133 | 반복 10 / 10 | 손실 0.27\n",
      "| 에폭 134 | 반복 10 / 10 | 손실 0.27\n",
      "| 에폭 135 | 반복 10 / 10 | 손실 0.27\n",
      "| 에폭 136 | 반복 10 / 10 | 손실 0.26\n",
      "| 에폭 137 | 반복 10 / 10 | 손실 0.26\n",
      "| 에폭 138 | 반복 10 / 10 | 손실 0.26\n",
      "| 에폭 139 | 반복 10 / 10 | 손실 0.25\n",
      "| 에폭 140 | 반복 10 / 10 | 손실 0.24\n",
      "| 에폭 141 | 반복 10 / 10 | 손실 0.24\n",
      "| 에폭 142 | 반복 10 / 10 | 손실 0.25\n",
      "| 에폭 143 | 반복 10 / 10 | 손실 0.24\n",
      "| 에폭 144 | 반복 10 / 10 | 손실 0.24\n",
      "| 에폭 145 | 반복 10 / 10 | 손실 0.23\n",
      "| 에폭 146 | 반복 10 / 10 | 손실 0.24\n",
      "| 에폭 147 | 반복 10 / 10 | 손실 0.23\n",
      "| 에폭 148 | 반복 10 / 10 | 손실 0.23\n",
      "| 에폭 149 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 150 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 151 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 152 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 153 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 154 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 155 | 반복 10 / 10 | 손실 0.22\n",
      "| 에폭 156 | 반복 10 / 10 | 손실 0.21\n",
      "| 에폭 157 | 반복 10 / 10 | 손실 0.21\n",
      "| 에폭 158 | 반복 10 / 10 | 손실 0.20\n",
      "| 에폭 159 | 반복 10 / 10 | 손실 0.21\n",
      "| 에폭 160 | 반복 10 / 10 | 손실 0.20\n",
      "| 에폭 161 | 반복 10 / 10 | 손실 0.20\n",
      "| 에폭 162 | 반복 10 / 10 | 손실 0.20\n",
      "| 에폭 163 | 반복 10 / 10 | 손실 0.21\n",
      "| 에폭 164 | 반복 10 / 10 | 손실 0.20\n",
      "| 에폭 165 | 반복 10 / 10 | 손실 0.20\n",
      "| 에폭 166 | 반복 10 / 10 | 손실 0.19\n",
      "| 에폭 167 | 반복 10 / 10 | 손실 0.19\n",
      "| 에폭 168 | 반복 10 / 10 | 손실 0.19\n",
      "| 에폭 169 | 반복 10 / 10 | 손실 0.19\n",
      "| 에폭 170 | 반복 10 / 10 | 손실 0.19\n",
      "| 에폭 171 | 반복 10 / 10 | 손실 0.19\n",
      "| 에폭 172 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 173 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 174 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 175 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 176 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 177 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 178 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 179 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 180 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 181 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 182 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 183 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 184 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 185 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 186 | 반복 10 / 10 | 손실 0.18\n",
      "| 에폭 187 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 188 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 189 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 190 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 191 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 192 | 반복 10 / 10 | 손실 0.17\n",
      "| 에폭 193 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 194 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 195 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 196 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 197 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 198 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 199 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 200 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 201 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 202 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 203 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 204 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 205 | 반복 10 / 10 | 손실 0.16\n",
      "| 에폭 206 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 207 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 208 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 209 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 210 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 211 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 212 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 213 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 214 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 215 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 216 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 217 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 218 | 반복 10 / 10 | 손실 0.15\n",
      "| 에폭 219 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 220 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 221 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 222 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 223 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 224 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 225 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 226 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 227 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 228 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 229 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 230 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 231 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 232 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 233 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 234 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 235 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 236 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 237 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 238 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 239 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 240 | 반복 10 / 10 | 손실 0.14\n",
      "| 에폭 241 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 242 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 243 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 244 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 245 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 246 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 247 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 248 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 249 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 250 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 251 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 252 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 253 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 254 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 255 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 256 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 257 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 258 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 259 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 260 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 261 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 262 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 263 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 264 | 반복 10 / 10 | 손실 0.13\n",
      "| 에폭 265 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 266 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 267 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 268 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 269 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 270 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 271 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 272 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 273 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 274 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 275 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 276 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 277 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 278 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 279 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 280 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 281 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 282 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 283 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 284 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 285 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 286 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 287 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 288 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 289 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 290 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 291 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 292 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 293 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 294 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 295 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 296 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 297 | 반복 10 / 10 | 손실 0.12\n",
      "| 에폭 298 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 299 | 반복 10 / 10 | 손실 0.11\n",
      "| 에폭 300 | 반복 10 / 10 | 손실 0.11\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "# 데이터 읽기, 모델과 옵티마이저 생성\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 학습에 사용하는 변수\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 데이터 뒤섞기\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        \n",
    "        # 기울기를 구해 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        # 정기적으로 학습 경과 출력\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| 에폭 %d | 반복 %d / %d | 손실 %.2f'\n",
    "                 % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 2 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 3 |  반복 1 / 10 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 4 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 5 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 6 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 7 |  반복 1 / 10 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 8 |  반복 1 / 10 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 9 |  반복 1 / 10 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 10 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 11 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 12 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 13 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 14 |  반복 1 / 10 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 15 |  반복 1 / 10 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 16 |  반복 1 / 10 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 17 |  반복 1 / 10 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 18 |  반복 1 / 10 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 19 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 20 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 21 |  반복 1 / 10 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 22 |  반복 1 / 10 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 23 |  반복 1 / 10 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 24 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 25 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 26 |  반복 1 / 10 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 27 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 28 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 29 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 30 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 31 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 32 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 33 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 34 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 35 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 36 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 37 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 38 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 39 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 40 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 41 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 42 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 43 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 44 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 45 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 46 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 47 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 48 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 49 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 50 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 51 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 52 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 53 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 54 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 55 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 56 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 57 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 58 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 59 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 60 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 61 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 62 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 63 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 64 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 65 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 66 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 67 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 68 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 69 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 70 |  반복 1 / 10 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 71 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 72 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 73 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 74 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 75 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 76 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 77 |  반복 1 / 10 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 78 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 79 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 80 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 81 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 82 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 83 |  반복 1 / 10 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 84 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 85 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 86 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 87 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 88 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 89 |  반복 1 / 10 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 90 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 91 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 92 |  반복 1 / 10 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 93 |  반복 1 / 10 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 94 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 95 |  반복 1 / 10 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 96 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 97 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 98 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 99 |  반복 1 / 10 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 100 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 101 |  반복 1 / 10 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 102 |  반복 1 / 10 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 103 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 104 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 105 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 106 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 107 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 108 |  반복 1 / 10 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 109 |  반복 1 / 10 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 110 |  반복 1 / 10 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 111 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 112 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 113 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 114 |  반복 1 / 10 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 115 |  반복 1 / 10 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 116 |  반복 1 / 10 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 117 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 118 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 119 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 120 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 121 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 122 |  반복 1 / 10 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 123 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 124 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 125 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 126 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 127 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 128 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 129 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 130 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 131 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 132 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 133 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 134 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 135 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 136 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 137 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 138 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 139 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 140 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 141 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 142 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 143 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 144 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 145 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 146 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 147 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 148 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 149 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 150 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 151 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 152 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 153 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 154 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 155 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 156 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 157 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 158 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 159 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 160 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 161 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 162 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 163 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 164 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 165 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 166 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 167 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 168 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 169 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 170 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 171 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 172 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 173 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 174 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 175 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 176 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 177 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 178 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 179 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 180 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 181 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 182 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 183 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 184 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 185 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 186 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 187 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 188 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 189 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 190 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 191 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 192 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 193 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 194 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 195 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 196 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 197 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 198 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 199 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 200 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 201 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 202 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 203 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 204 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 205 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 206 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 207 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 208 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 209 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 210 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 211 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 212 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 213 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 214 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 215 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 216 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 217 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 218 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 219 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 220 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 221 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 222 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 223 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 224 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 225 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 226 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 227 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 228 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 229 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 230 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 231 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 232 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 233 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 234 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 235 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 236 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 237 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 238 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 239 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 240 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 241 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 242 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 243 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 244 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 245 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 246 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 247 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 248 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 249 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 250 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 251 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 252 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 253 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 254 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 255 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 256 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 257 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 258 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 259 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 260 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 261 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 262 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 263 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 264 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 265 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 266 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 267 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 268 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 269 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 270 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 271 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 272 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 273 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 274 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 275 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 276 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 277 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 278 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 279 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 280 |  반복 1 / 10 | 시간 0[s] | 손실 0.10\n",
      "| 에폭 281 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 282 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 283 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 284 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 285 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 286 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 287 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 288 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 289 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 290 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 291 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 292 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 293 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 294 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 295 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 296 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 297 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 298 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 299 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 300 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import Trainer\n",
    "from dataset import spiral\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
    "trainer.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
